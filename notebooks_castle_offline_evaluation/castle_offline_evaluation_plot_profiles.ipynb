{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e12c5fb-bdc3-455a-96b1-3c685c16b014",
   "metadata": {},
   "source": [
    "# Offline diagnostics for the CASTLE single output networks following Rasp et al. (2018) architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681699c5-4496-426b-9631-76b19da91b02",
   "metadata": {},
   "source": [
    "# Profile Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80663586-b976-4036-bf61-041ac57da701",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de40201b-2959-4f57-9333-4c61fd4e5c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printe\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8d0f2ce-f43e-480e-a529-5d1871609cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "# Relative imports\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199be6c2-7473-4798-acf8-e0bc5b289502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caea0847-9d0e-4704-85a2-6550d7a921e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "from utils.setup import SetupDiagnostics\n",
    "from neural_networks.load_models import load_models, get_save_plot_folder\n",
    "from neural_networks.model_diagnostics import ModelDiagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329a0d6e-1643-405e-a259-8deb682948df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a6faf5-18dc-4a5f-8106-3b9a0300ed09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adec015f-d0c4-4eda-ae03-1fbc8ef29eab",
   "metadata": {},
   "source": [
    "## Load trained CASTLE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4909bf5d-3e49-41a3-acc8-10a596a395c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "argv  = [\"-c\", \"../output_castle/training_6_normal/cfg_castle_NN_Creation.yml\"]\n",
    "plot_dir = Path(\"../output_castle/training_6_normal/plots_offline_evaluation/plot_profiles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8c0764f-66dd-4ebc-a1de-48376d040b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "castle_setup = SetupDiagnostics(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56f4b85e-f044-4ff4-904e-97c427ffccc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_0_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_1_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_2_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_3_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_4_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_5_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_6_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_7_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_8_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_9_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_10_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_11_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_12_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_13_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_14_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_15_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_16_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_17_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_18_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_19_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_20_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_21_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_22_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_23_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_24_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_25_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_26_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_27_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_28_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/1_29_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_0_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_1_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_2_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_3_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_4_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_5_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_6_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_7_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_8_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_9_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_10_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_11_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_12_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_13_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_14_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_15_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_16_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_17_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_18_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_19_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_20_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_21_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_22_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_23_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_24_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_25_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_26_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_27_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_28_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/0_29_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/2_0_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/3_0_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/4_0_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/5_0_model.h5\n",
      "\n",
      "Load model: /work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/output_castle/training_6_normal/models_castle/castleNN/r1.0-a1.0-b1.0-l1.0/hl_256_256_256_256_256_256_256_256_256-act_ReLU-e_18/6_0_model.h5\n"
     ]
    }
   ],
   "source": [
    "castle_models = load_models(castle_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63903c6b-30f6-45b8-814c-3dcafdd324b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(castle_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd816040-95d0-4ae1-8cb9-419edfa1f061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(castle_models['castleNN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1e649c-4be5-4d8c-91a6-40886b2f841b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tphystnd-3.64', 'tphystnd-7.59', 'tphystnd-14.36', 'tphystnd-24.61', 'tphystnd-38.27', 'tphystnd-54.6', 'tphystnd-72.01', 'tphystnd-87.82', 'tphystnd-103.32', 'tphystnd-121.55', 'tphystnd-142.99', 'tphystnd-168.23', 'tphystnd-197.91', 'tphystnd-232.83', 'tphystnd-273.91', 'tphystnd-322.24', 'tphystnd-379.1', 'tphystnd-445.99', 'tphystnd-524.69', 'tphystnd-609.78', 'tphystnd-691.39', 'tphystnd-763.4', 'tphystnd-820.86', 'tphystnd-859.53', 'tphystnd-887.02', 'tphystnd-912.64', 'tphystnd-936.2', 'tphystnd-957.49', 'tphystnd-976.33', 'tphystnd-992.56', 'phq-3.64', 'phq-7.59', 'phq-14.36', 'phq-24.61', 'phq-38.27', 'phq-54.6', 'phq-72.01', 'phq-87.82', 'phq-103.32', 'phq-121.55', 'phq-142.99', 'phq-168.23', 'phq-197.91', 'phq-232.83', 'phq-273.91', 'phq-322.24', 'phq-379.1', 'phq-445.99', 'phq-524.69', 'phq-609.78', 'phq-691.39', 'phq-763.4', 'phq-820.86', 'phq-859.53', 'phq-887.02', 'phq-912.64', 'phq-936.2', 'phq-957.49', 'phq-976.33', 'phq-992.56', 'fsnt', 'fsns', 'flnt', 'flns', 'prect'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: keys are variables not strings\n",
    "castle_models['castleNN'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff2444-c218-418b-ac41-e473e7b72f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b31e8a0-944a-4960-b1b6-36086ce65bea",
   "metadata": {},
   "source": [
    "## Vertical cross-section plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f974457f-d977-47f4-a154-d0368853e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable does not exist in the code (but key nn_type is the same)\n",
    "castle_model_type = \"castleNN\"\n",
    "castle_setup.model_type = castle_model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71536011-6de3-4246-b639-de2aa7c02988",
   "metadata": {},
   "outputs": [],
   "source": [
    "castle_md = ModelDiagnostics(setup=castle_setup, \n",
    "                             models=castle_models[castle_model_type]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9156eba5-d684-4549-b2a9-0311d1ab9f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neural_networks.model_diagnostics.ModelDiagnostics at 0x7ff9403e8f70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "castle_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e3532d-8051-4c44-89ce-e45420722134",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keys = castle_models['castleNN'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118fcc29-b445-4807-8e64-cdbecc0d7206",
   "metadata": {},
   "source": [
    "### Single Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c752dcac-8e72-48e5-97cb-25c460ed19a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.variable import Variable_Lev_Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e1e170b-176f-494d-b99b-20fd538926a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = Variable_Lev_Metadata.parse_var_name(\"tphystnd-0\")\n",
    "var_keys = [k for k in dict_keys if var.var.value in str(k)]\n",
    "unit = \"K/s\"\n",
    "# phq unit \"kg/(kg*s)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ee0ea93-0abc-4bd6-a4ba-09736293b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting double_profiles for variable tphystnd\n",
      "\n",
      "Validation batch size = 8192.\n",
      "Time samples: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 16:19:10.658962: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_92/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:19:10.669283: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:19:10.670223: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:19:20.680184: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_88/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:19:20.691724: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:19:20.694819: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:19:30.699752: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_28/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:19:30.710064: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:19:30.711358: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:19:40.721780: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_21/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:19:40.733237: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:19:40.736657: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:19:50.740762: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_31/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:19:50.751154: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:19:50.752116: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:20:00.762809: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_24/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:20:00.774413: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:20:00.777605: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:20:10.782157: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_16/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:20:10.792601: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:20:10.793901: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:20:20.804651: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_27/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:20:20.816325: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:20:20.819401: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:20:30.824885: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_20/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:20:30.835046: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:20:30.836041: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:20:40.841928: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_23/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:20:40.852053: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:20:40.852108: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:20:50.858188: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_94/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:20:50.869760: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:20:50.874080: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:21:00.884649: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_81/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:21:00.894739: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:21:00.894796: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:21:10.900070: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_78/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:21:10.910623: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:21:10.914157: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:21:20.925355: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_75/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:21:20.935460: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:21:20.935518: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:21:30.935774: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_58/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:21:30.946368: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:21:30.947702: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:21:40.953233: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_49/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:21:40.963346: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:21:40.963402: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:21:50.969035: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_43/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:21:50.980613: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:21:50.984382: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:22:00.994743: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_39/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:22:01.004843: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:22:01.004900: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:22:11.016418: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_34/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:22:11.027898: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:22:11.032188: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:22:21.042609: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_30/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:22:21.052781: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:22:21.052839: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:22:31.055517: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_26/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:22:31.066557: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:22:31.070033: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:22:41.079998: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_19/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:22:41.090114: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:22:41.090171: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:22:51.095961: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_12/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:22:51.107409: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:22:51.111800: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:23:01.122283: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_9/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:23:01.132413: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:23:01.132475: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:23:11.132741: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_5/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:23:11.143199: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:23:11.143225: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:23:21.149605: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_1/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:23:21.160417: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:23:21.163945: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:23:31.185105: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_22/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:23:31.195620: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:23:31.195679: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:23:41.198579: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_15/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:23:41.208707: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:23:41.209958: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:23:51.218957: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_8/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:23:51.230000: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:23:51.231776: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:24:01.238389: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_4/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:24:01.248501: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:24:01.249400: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:24:11.260371: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_18/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:24:11.271926: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:24:11.276671: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:24:21.283707: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_42/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:24:21.294569: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:24:21.294638: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:24:31.294851: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_11/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:24:31.305403: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:24:31.305429: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:24:41.307991: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_0/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:24:41.318093: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:24:41.319119: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:24:51.328520: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_38/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:24:51.339980: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:24:51.342782: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:25:01.349683: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_3/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:25:01.359808: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:25:01.361051: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:25:11.369500: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_14/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:25:11.381119: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:25:11.384200: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:25:21.391847: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_10/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:25:21.401938: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:25:21.402893: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:25:31.405828: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_71/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:25:31.416186: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:25:31.416219: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:25:41.416654: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_64/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:25:41.427211: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:25:41.427229: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:25:51.435107: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_60/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:25:51.446949: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:25:51.451408: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:26:01.459144: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_93/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:26:01.469967: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:26:01.470027: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:26:11.478383: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_7/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:26:11.489743: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:26:11.492100: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:26:21.499370: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_68/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:26:21.509528: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:26:21.509595: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:26:31.510238: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_56/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:26:31.520394: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:26:31.520420: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:26:41.522756: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_2/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:26:41.533173: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:26:41.534461: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:26:51.542795: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_90/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:26:51.554532: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:26:51.557643: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:27:01.565181: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_87/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:27:01.575275: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:27:01.576189: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:27:11.584356: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_77/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:27:11.595761: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:27:11.598844: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:27:21.605936: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_74/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:27:21.616046: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:27:21.617260: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:27:31.619931: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_70/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:27:31.630632: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:27:31.630663: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:27:41.633054: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_61/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:27:41.643168: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:27:41.644122: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:27:51.652785: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_6/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:27:51.664435: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:27:51.667626: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:28:01.674997: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_91/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:28:01.685103: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:28:01.686296: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:28:11.697080: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_84/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:28:11.708722: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:28:11.712820: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:28:21.720656: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_80/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:28:21.731146: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:28:21.731172: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:28:31.731440: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_67/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:28:31.742194: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:28:31.742250: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:28:41.750459: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_63/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:28:41.762059: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:28:41.766425: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:28:51.773994: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_59/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:28:51.784693: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:28:51.784753: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:29:01.793002: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_52/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:29:01.804532: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:29:01.808555: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:29:11.816377: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_48/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:29:11.827021: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:29:11.827074: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:29:21.835175: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_41/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:29:21.846730: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:29:21.851100: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:29:31.858844: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_73/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:29:31.869590: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:29:31.869648: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:29:41.877643: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_69/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:29:41.889167: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:29:41.893151: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:29:51.906004: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_57/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:29:51.917732: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:29:51.920761: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:30:01.927963: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_50/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:30:01.938044: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:30:01.939239: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:30:11.948121: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_44/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:30:11.959698: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:30:11.962777: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:30:21.970303: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_37/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:30:21.980431: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:30:21.981381: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:30:31.984299: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_89/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:30:31.994981: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:30:31.995011: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:30:41.998246: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_85/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:30:42.008396: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:30:42.009672: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:30:52.018301: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_83/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:30:52.029667: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:30:52.032872: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:31:02.039992: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_79/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:31:02.050380: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:31:02.051354: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:31:12.061885: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_76/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:31:12.073505: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:31:12.076975: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:31:22.081255: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_66/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:31:22.092009: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:31:22.092037: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:31:32.095707: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_62/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:31:32.106789: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:31:32.110502: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:31:42.117361: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_55/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:31:42.127734: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:31:42.127752: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:31:52.135624: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_40/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:31:52.147172: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:31:52.151219: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:32:02.158996: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_33/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:32:02.169589: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:32:02.169606: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:32:12.176924: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/input_sub_layer_17/MatMul\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:32:12.187740: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:32:12.210876: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "2023-07-16 16:32:22.282110: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 8.00MiB (rounded to 8388608)requested by op castleNN/shared_hidden_layer_0/MatMul_81\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-07-16 16:32:22.292628: W tensorflow/tsl/framework/bfc_allocator.cc:497] ****************************************************************************************************\n",
      "2023-07-16 16:32:22.292646: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:730 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'castleNN/input_sub_layer_92/MatMul' defined at (most recent call last):\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1495777/3908021388.py\", line 4, in <module>\n      fig = castle_md. plot_double_profile(var, var_keys, itime=i_time, nTime=n_time,\n    File \"/work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/neural_networks/model_diagnostics.py\", line 623, in plot_double_profile\n      t, p = self.get_truth_pred(itime, varkeys[0], nTime=nTime)\n    File \"/work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/neural_networks/model_diagnostics.py\", line 137, in get_truth_pred\n      p_tmp = model.predict_on_batch(X_tmp[:, inputs])\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2603, in predict_on_batch\n      outputs = self.predict_function(iterator)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/layers/core/dense.py\", line 241, in call\n      outputs = tf.matmul(a=inputs, b=self.kernel)\nNode: 'castleNN/input_sub_layer_92/MatMul'\nOOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node castleNN/input_sub_layer_92/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_966703]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m n_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      3\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mcastle_md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mplot_double_profile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mlats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlons\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m359.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/neural_networks/model_diagnostics.py:623\u001b[0m, in \u001b[0;36mModelDiagnostics.plot_double_profile\u001b[0;34m(self, var, varkeys, itime, nTime, lats, lons, save, stats, show_plot, unit, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m     nTime \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 623\u001b[0m     t, p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_truth_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnTime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m     nTime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(t)\n\u001b[1;32m    626\u001b[0m idx_lats \u001b[38;5;241m=\u001b[39m [find_closest_value(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatitudes, lat) \u001b[38;5;28;01mfor\u001b[39;00m lat \u001b[38;5;129;01min\u001b[39;00m lats]\n",
      "File \u001b[0;32m/work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/neural_networks/model_diagnostics.py:137\u001b[0m, in \u001b[0;36mModelDiagnostics.get_truth_pred\u001b[0;34m(self, itime, var, nTime)\u001b[0m\n\u001b[1;32m    135\u001b[0m X_tmp \u001b[38;5;241m=\u001b[39m valid_gen[iTime]\n\u001b[1;32m    136\u001b[0m t_tmp, _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msplit(X_tmp, [\u001b[38;5;241m1\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m p_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# We only want the prediction for Y, not the reconstruction of the X's\u001b[39;00m\n\u001b[1;32m    139\u001b[0m p_tmp \u001b[38;5;241m=\u001b[39m p_tmp[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py:2603\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2599\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2600\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x\n\u001b[1;32m   2601\u001b[0m     )\n\u001b[1;32m   2602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_predict_function()\n\u001b[0;32m-> 2603\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(outputs)\n",
      "File \u001b[0;32m/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'castleNN/input_sub_layer_92/MatMul' defined at (most recent call last):\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1495777/3908021388.py\", line 4, in <module>\n      fig = castle_md. plot_double_profile(var, var_keys, itime=i_time, nTime=n_time,\n    File \"/work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/neural_networks/model_diagnostics.py\", line 623, in plot_double_profile\n      t, p = self.get_truth_pred(itime, varkeys[0], nTime=nTime)\n    File \"/work/bd1179/b309247/pycharm_projects/iglesias-suarez2yxx_spuriouslinks/neural_networks/model_diagnostics.py\", line 137, in get_truth_pred\n      p_tmp = model.predict_on_batch(X_tmp[:, inputs])\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2603, in predict_on_batch\n      outputs = self.predict_function(iterator)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2169, in predict_function\n      return step_function(self, iterator)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2155, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2143, in run_step\n      outputs = model.predict_step(data)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in predict_step\n      return self(x, training=False)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/work/bd1179/b309247/miniconda3/envs/tensorflow_env/lib/python3.9/site-packages/keras/layers/core/dense.py\", line 241, in call\n      outputs = tf.matmul(a=inputs, b=self.kernel)\nNode: 'castleNN/input_sub_layer_92/MatMul'\nOOM when allocating tensor with shape[8192,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node castleNN/input_sub_layer_92/MatMul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_predict_function_966703]"
     ]
    }
   ],
   "source": [
    "i_time = \"range\"\n",
    "n_time = 5\n",
    "stats = \"mse\"\n",
    "fig = castle_md. plot_double_profile(var, var_keys, itime=i_time, nTime=n_time, \n",
    "                                     lats=[-90, 90], lons=[0., 359.], save=False, \n",
    "                                     stats=stats, show_plot=True, unit=unit)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b8f08-4be4-4953-a39b-d9b70b43c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_time = \"range\"\n",
    "n_time = 5\n",
    "stats = \"r2\"\n",
    "fig = castle_md. plot_double_profile(var, var_keys, itime=i_time, nTime=n_time, \n",
    "                                     lats=[-90, 90], lons=[0., 359.], save=False, \n",
    "                                     stats=stats, show_plot=True, unit=unit)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a93a1b-8499-4ce5-8c57-b2e83c26db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_time = \"range\"\n",
    "n_time = 5\n",
    "stats = \"r2\"\n",
    "_ = castle_md.plot_double_profile(var, var_keys, itime=i_time, nTime=n_time, \n",
    "                                     lats=[-90, 90], lons=[0., 359.], save=plot_dir, \n",
    "                                     stats=stats, show_plot=False, unit=unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a693c4ea-a45f-4fba-adbb-266ab1de0214",
   "metadata": {
    "tags": []
   },
   "source": [
    "### All variables 3d variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c1fc7-143f-4a05-a4c0-eccbca6e8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_str(idx_time, num_time=False, latitudes=False, \n",
    "                 longitudes=False, statistics=False):\n",
    "    if type(idx_time) is int:\n",
    "        idx_time_str = f\"step-{idx_time}\"\n",
    "    elif type(idx_time) is str:\n",
    "        if num_time:\n",
    "            idx_time_str = f\"{idx_time}-{num_time}\"\n",
    "        else:\n",
    "            idx_time_str = f\"{idx_time}-all\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unkown value for idx_time: {idx_time}\")\n",
    "\n",
    "    lats_str = f\"_lats_{latitudes[0]}_{latitudes[1]}\" if latitudes else \"\"\n",
    "    lons_str = f\"_lats_{longitudes[0]}_{longitudes[1]}n\" if longitudes else \"\"\n",
    "    stats_str =  f\"_stats-{statistics}\" if statistics else \"\"\n",
    "    \n",
    "    return idx_time_str + lats_str + lons_str + stats_str\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe15dc8-4015-4367-9cc8-1a0a8202e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 3d\n",
    "# tphystnd unit \"K/s\"\n",
    "# phq unit \"kg/(kg*s)\"\n",
    "var_unit_str_three_d = [(\"tphystnd-3.64\", \"K/s\"), (\"phq-3.64\", \"kg/(kg*s)\")] \n",
    "three_d_keys = [(Variable_Lev_Metadata.parse_var_name(var_str), unit) for var_str, unit in var_unit_str_three_d]\n",
    "\n",
    "dict_keys = castle_models['castleNN'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43203b62-0dc4-4aa3-a5df-0de4ebea50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not function parameters, uses variables that are set in Notebook cells!!\n",
    "def run_plot_profiles():\n",
    "    save_dir = Path(plot_dir, get_save_str(i_time, num_time=n_time, latitudes=lats, \n",
    "                                           longitudes=lons, statistics=stats))\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    for var, unit in three_d_keys:\n",
    "        print(var)\n",
    "        var_keys = [k for k in dict_keys if var.var.value in str(k)]\n",
    "        \n",
    "        _ = castle_md.plot_double_profile(var, var_keys, itime=i_time, nTime=n_time, \n",
    "                                          lats=lats, lons=lons, save=save_dir, \n",
    "                                          stats=stats, show_plot=False, unit=unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d2c2cd-6b4b-42ab-b296-38b730d75347",
   "metadata": {},
   "source": [
    "#### Time range, 5 steps, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bcdb4-d7df-405d-83ec-f5ac2d4684bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i_time = \"range\"\n",
    "n_time = 5\n",
    "lats=[-90, 90]\n",
    "lons=[0., 359.]\n",
    "stats = \"mse\"\n",
    "\n",
    "run_plot_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f467c5-ecbf-4782-99bf-92ee410933ec",
   "metadata": {},
   "source": [
    "#### Time range, 5 steps, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5475a32e-b486-4793-a559-9ce94eb9f626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i_time = \"range\"\n",
    "n_time = 5\n",
    "lats=[-90, 90]\n",
    "lons=[0., 359.]\n",
    "stats = \"r2\"\n",
    "\n",
    "run_plot_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9369e372-8a63-4a81-8ff3-4a62b2f003fe",
   "metadata": {},
   "source": [
    "#### Time range, 1440 steps, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77192e-e58e-4334-9ae9-826ce5edafe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Time step CAM=30min\n",
    "# Time step SRM=20sec\n",
    "# (30*60sec)/20sec = 90 \n",
    "# Or: 120 as Nando did\n",
    "i_time = \"range\"\n",
    "n_time = 1440\n",
    "lats=[-90, 90]\n",
    "lons=[0., 359.]\n",
    "# stats = \"mse\" --> does not work with stats turned on \n",
    "\n",
    "run_plot_profiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86c625-c59a-475b-ab99-ffa1bf4782db",
   "metadata": {},
   "source": [
    "#### Time range, 1440 steps, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622a3eb-b3f3-42c7-89e3-caa3de8eb55a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Time step CAM=30min\n",
    "# Time step SRM=20sec\n",
    "# (30*60sec)/20sec = 90 \n",
    "# Or: 120 as Nando did\n",
    "i_time = \"range\"\n",
    "n_time = 1440\n",
    "lats=[-90, 90]\n",
    "lons=[0., 359.]\n",
    "stats = \"r2\"\n",
    "\n",
    "run_plot_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962413b-baf7-46a6-b989-3ea88940af74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow Kernel",
   "language": "python",
   "name": "tensorflow-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
